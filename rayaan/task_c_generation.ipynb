{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d73896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rayaa\\anaconda3\\envs\\rag_gen_eval\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_postgres\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PGVector\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db0a993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths and models configured\n"
     ]
    }
   ],
   "source": [
    "# Paths setup\n",
    "BASE = Path('c:/Users/rayaa/OneDrive/Documents/VSCode/CSCI5832/Semeval')\n",
    "RAG_TASKS_PATH = BASE / 'human' / 'generation_tasks' / 'RAG.jsonl'\n",
    "CORPUS_PATH = BASE / 'corpora' / 'passage_level' / 'cloud.jsonl'\n",
    "\n",
    "# Model and database setup\n",
    "EMBED_MODEL = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "GENERATION_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "PG_ENV_PATH = BASE / '.pg_env'\n",
    "\n",
    "print('Paths and models configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9cc605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 205 filtered RAG tasks\n"
     ]
    }
   ],
   "source": [
    "def load_rag_tasks(jsonl_path, collection_name=\"mt-rag-ibmcloud-elser-512-100-20240502\"):\n",
    "    \"\"\"Load RAG tasks from JSONL file, filtering by id.Collection.\"\"\"\n",
    "    tasks = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Safely extract Collection field\n",
    "            collection = obj.get(\"Collection\", \"\")\n",
    "\n",
    "            if collection == collection_name:\n",
    "                tasks.append(obj)\n",
    "\n",
    "    return tasks\n",
    "\n",
    "# Load RAG tasks\n",
    "rag_tasks = load_rag_tasks(RAG_TASKS_PATH)\n",
    "print(f\"Loaded {len(rag_tasks)} filtered RAG tasks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6956900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Postgres connection from c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\.pg_env\n",
      "Initializing PGVector connection...\n",
      "PGVector ready\n"
     ]
    }
   ],
   "source": [
    "# Setup retrieval system (similar to Task A)\n",
    "vectorstore = None\n",
    "connection_string = None\n",
    "if PG_ENV_PATH.exists():\n",
    "    print(f'Loading Postgres connection from {PG_ENV_PATH}')\n",
    "    load_dotenv(PG_ENV_PATH)\n",
    "    connection_string = os.getenv('PG_CONNECTION_STRING')\n",
    "\n",
    "if connection_string:\n",
    "    print('Initializing PGVector connection...')\n",
    "    hf_emb = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "    vectorstore = PGVector(connection=connection_string, embeddings=hf_emb)\n",
    "    print('PGVector ready')\n",
    "else:\n",
    "    print('No PG connection found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66da8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conversation_text(task):\n",
    "    \"\"\"Extract the current question from conversation input\"\"\"\n",
    "    input_data = task.get('input', [])\n",
    "    if isinstance(input_data, list) and len(input_data) > 0:\n",
    "        # Get the last user message\n",
    "        for msg in reversed(input_data):\n",
    "            if msg.get('speaker') == 'user':\n",
    "                return msg.get('text', '')\n",
    "    return ''\n",
    "\n",
    "def retrieve_passages(query_text, top_k=5):\n",
    "    \"\"\"Retrieve top K passages for a query\"\"\"\n",
    "    if vectorstore is None:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        docs_and_scores = vectorstore.similarity_search_with_score(query_text, k=top_k)\n",
    "        retrieved = []\n",
    "        for doc, distance_score in docs_and_scores:\n",
    "            # Extract document ID\n",
    "            docid = None\n",
    "            if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):\n",
    "                docid = doc.metadata.get('doc_id') or doc.metadata.get('docid') or doc.metadata.get('id')\n",
    "            if not docid:\n",
    "                docid = getattr(doc, 'id', None) or getattr(doc, 'page_content', '')[:64]\n",
    "            \n",
    "            similarity_score = 1 - distance_score  # Convert distance to similarity\n",
    "            retrieved.append({\n",
    "                'document_id': str(docid),\n",
    "                'text': doc.page_content,\n",
    "                'score': float(similarity_score)\n",
    "            })\n",
    "        \n",
    "        return retrieved\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving for query: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d942db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator setup complete\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "def setup_generator(model_name=GENERATION_MODEL):\n",
    "    \"\"\"Setup the text generation model\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return generator\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up generator: {e}\")\n",
    "        return None\n",
    "\n",
    "# Setup the generator\n",
    "generator = setup_generator()\n",
    "print('Generator setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3147335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generation_prompt(question, contexts, conversation_history=None):\n",
    "    \"\"\"Create a prompt for answer generation using retrieved contexts\"\"\"\n",
    "    \n",
    "    # Build context string\n",
    "    context_text = \"\"\n",
    "    for i, ctx in enumerate(contexts, 1):\n",
    "        context_text += f\"Context {i}: {ctx['text']}\\n\\n\"\n",
    "    \n",
    "    # Build conversation history if available\n",
    "    history = \"\"\n",
    "    if conversation_history:\n",
    "        turns = [\n",
    "            f\"{t['speaker'].capitalize()}: {t['text']}\"\n",
    "            for t in conversation_history\n",
    "        ]\n",
    "        history = \"Conversation:\\n\" + \"\\n\".join(turns) + \"\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"You are a concise conversational assistant.\n",
    "\n",
    "Use ONLY the information found in the contexts.  \n",
    "If the answer is not in the contexts, say exactly: **\"The contexts do not contain the answer.\"**\n",
    "\n",
    "Rules:\n",
    "- Do NOT explain your reasoning.\n",
    "- Do NOT mention the instructions.\n",
    "- Do NOT invent information.\n",
    "- Answer in a single short, natural paragraph (1â€“2 sentences).\n",
    "- No meta-commentary (e.g., \"I will now...\").\n",
    "\n",
    "{history}Contexts:\n",
    "{context_text}\n",
    "\n",
    "User: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_answer(prompt, generator):\n",
    "    \"\"\"Generate answer using the language model\"\"\"\n",
    "    if generator is None:\n",
    "        return \"[Generation model not available]\"\n",
    "    \n",
    "    try:\n",
    "        outputs = generator(\n",
    "            prompt,\n",
    "            return_full_text=False,\n",
    "            pad_token_id=generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        if outputs and len(outputs) > 0:\n",
    "            return outputs[0]['generated_text'].strip()\n",
    "        else:\n",
    "            return \"[No response generated]\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generation: {e}\")\n",
    "        return f\"[Generation error: {e}]\"\n",
    "    \n",
    "def trim_to_token_limit(text, tokenizer, max_length=4000):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) <= max_length:\n",
    "        return text\n",
    "    # Keep last max_length tokens so the question stays\n",
    "    trimmed = tokenizer.decode(tokens[-max_length:])\n",
    "    return trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af276f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a9cbb7d68540ddb7d2b62c5a7ad9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing RAG tasks:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 results to c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_c_rag_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "def run_task_c_rag(tasks, generator, output_path, do_subset=False):\n",
    "    \"\"\"Run full Task C pipeline: retrieval + generation\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if (do_subset):\n",
    "        tasks = tasks[:5]\n",
    "    \n",
    "    for task in tqdm(tasks, desc=\"Processing RAG tasks\"):\n",
    "        # Extract current question\n",
    "        current_question = extract_conversation_text(task)\n",
    "        \n",
    "        if not current_question:\n",
    "            print(f\"Warning: No question found for task {task.get('task_id')}\")\n",
    "            continue\n",
    "        \n",
    "        # Retrieve passages\n",
    "        retrieved_contexts = retrieve_passages(current_question, top_k=5)\n",
    "        \n",
    "        # Get conversation history (all but last user message)\n",
    "        conversation_history = []\n",
    "        input_data = task.get('input', [])\n",
    "        if isinstance(input_data, list):\n",
    "            # Include all but the last user message (current question)\n",
    "            found_last_user = False\n",
    "            for msg in reversed(input_data):\n",
    "                if msg.get('speaker') == 'user' and not found_last_user:\n",
    "                    found_last_user = True\n",
    "                    continue\n",
    "                conversation_history.insert(0, msg)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = create_generation_prompt(current_question, retrieved_contexts, conversation_history)\n",
    "        prompt = trim_to_token_limit(prompt, generator.tokenizer, max_length=4000)\n",
    "        generated_answer = generate_answer(prompt, generator)\n",
    "        \n",
    "        # Prepare result in evaluation format\n",
    "        result_task = task.copy()\n",
    "        \n",
    "        # Replace contexts with retrieved ones\n",
    "        result_task['contexts'] = retrieved_contexts\n",
    "        \n",
    "        # Add prediction in the format expected by evaluation script\n",
    "        result_task['predictions'] = [{\n",
    "            'text': generated_answer\n",
    "        }]\n",
    "        \n",
    "        results.append(result_task)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(output_path).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(results)} results to {output_path}\")\n",
    "    return results\n",
    "\n",
    "# Run Task C\n",
    "output_file = BASE / 'rayaan' / 'outputs' / 'task_c_rag_predictions.jsonl'\n",
    "task_c_results = run_task_c_rag(rag_tasks, generator, output_file, do_subset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88709262",
   "metadata": {},
   "source": [
    "**Note**: This script doesn't work due to some versioning differences in some packages between requirement files. The provided evaluation script also makes use of flash attention which doesn't seem to have a functioning wheel installer at the moment. I have tried the tips posted in the coordinator's readme, but I still run into issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b725ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING GENERATION EVALUATION PIPELINE\n",
      "============================================================\n",
      "ðŸ“ Input file: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_c_rag_predictions.jsonl (5 tasks)\n",
      "ðŸ“ Output directory: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\n",
      "ðŸ”§ Evaluation script: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\scripts\\evaluation\\run_generation_eval.py\n",
      "ðŸ¤– Using HF judge model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "ðŸš€ Command: python c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\scripts\\evaluation\\run_generation_eval.py --input c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_c_rag_predictions.jsonl --output c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_c_evaluation_results.jsonl --algorithmic_evaluators c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\scripts\\evaluation\\config.yaml --provider hf --judge_model Qwen/Qwen2.5-0.5B-Instruct\n",
      "\n",
      "â³ Starting evaluation... This may take a while...\n",
      "â±ï¸  Execution time: 828.47 seconds (13.81 minutes)\n",
      "\n",
      "========================================\n",
      "EVALUATION RESULTS\n",
      "========================================\n",
      "ðŸ“‹ Script output:\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\bert_score\\\\default\\\\bs.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "Error deleting file: [WinError 5] Access is denied: 'C:\\\\Users\\\\rayaa\\\\.cache\\\\huggingface\\\\metrics\\\\rouge\\\\default\\\\rme.20251209155433-1-0.arrow'\n",
      "\n",
      "âœ… Evaluation completed successfully!\n",
      "ðŸ“Š Results: 5/5 tasks evaluated\n",
      "ðŸ’¾ Results saved to: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_c_evaluation_results.jsonl\n",
      "\n",
      "ðŸŽ‰ Task C evaluation pipeline completed successfully!\n",
      "You can now run the analysis cells to see the results.\n"
     ]
    }
   ],
   "source": [
    "output_file = BASE / 'rayaan' / 'outputs' / 'task_c_rag_predictions.jsonl'\n",
    "\n",
    "def run_generation_evaluation_script_advanced(input_file, output_file, provider=\"hf\", judge_model=None, openai_key=None, azure_host=None):\n",
    "    \"\"\"Run the generation evaluation pipeline with comprehensive error handling\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING GENERATION EVALUATION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Validate inputs\n",
    "    input_path = Path(input_file)\n",
    "    if not input_path.exists():\n",
    "        print(f\"âŒ ERROR: Input file not found at {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Count input tasks\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        input_count = sum(1 for line in f if line.strip())\n",
    "    print(f\"ðŸ“ Input file: {input_path} ({input_count} tasks)\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_path = Path(output_file)\n",
    "    output_dir = output_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"ðŸ“ Output directory: {output_dir}\")\n",
    "    \n",
    "    # Locate evaluation script\n",
    "    eval_script_path = BASE / 'scripts' / 'evaluation' / 'run_generation_eval.py'\n",
    "    if not eval_script_path.exists():\n",
    "        print(f\"âŒ ERROR: Evaluation script not found at {eval_script_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Locate config file\n",
    "    config_path = BASE / 'scripts' / 'evaluation' / 'config.yaml'\n",
    "    if not config_path.exists():\n",
    "        print(f\"âŒ ERROR: Config file not found at {config_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ðŸ”§ Evaluation script: {eval_script_path}\")\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [\n",
    "        'python', str(eval_script_path),\n",
    "        '--input', str(input_path),\n",
    "        '--output', str(output_path),\n",
    "        '--algorithmic_evaluators', str(config_path),\n",
    "        '--provider', provider\n",
    "    ]\n",
    "    \n",
    "    # Add provider-specific arguments\n",
    "    if provider == \"hf\" and judge_model:\n",
    "        cmd.extend(['--judge_model', judge_model])\n",
    "        print(f\"ðŸ¤– Using HF judge model: {judge_model}\")\n",
    "    elif provider == \"openai\":\n",
    "        if openai_key:\n",
    "            cmd.extend(['--openai_key', openai_key])\n",
    "            print(\"ðŸ”‘ OpenAI key provided\")\n",
    "        if azure_host:\n",
    "            cmd.extend(['--azure_host', azure_host])\n",
    "            print(f\"ðŸŒ Azure host: {azure_host}\")\n",
    "    \n",
    "    print(f\"ðŸš€ Command: {' '.join(cmd)}\")\n",
    "    print(\"\\nâ³ Starting evaluation... This may take a while...\")\n",
    "    \n",
    "    try:\n",
    "        # Run the evaluation script with timeout\n",
    "        import subprocess\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200)  # 2 hour timeout\n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"â±ï¸  Execution time: {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)\")\n",
    "        \n",
    "        # Process results\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if result.stdout:\n",
    "            print(\"ðŸ“‹ Script output:\")\n",
    "            print(result.stdout)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Evaluation completed successfully!\")\n",
    "            \n",
    "            # Verify output file\n",
    "            if output_path.exists():\n",
    "                with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                    output_count = sum(1 for line in f if line.strip())\n",
    "                print(f\"ðŸ“Š Results: {output_count}/{input_count} tasks evaluated\")\n",
    "                print(f\"ðŸ’¾ Results saved to: {output_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ ERROR: Output file was not created\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"âŒ Evaluation failed with return code: {result.returncode}\")\n",
    "            if result.stderr:\n",
    "                print(\"ðŸ”´ Error details:\")\n",
    "                print(result.stderr)\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° ERROR: Evaluation script timed out after 2 hours\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ’¥ Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run evaluation on Task C results\n",
    "eval_output_file = BASE / 'rayaan' / 'outputs' / 'task_c_evaluation_results.jsonl'\n",
    "\n",
    "# Configuration - Choose your setup\n",
    "provider = \"hf\"  # Options: \"hf\" or \"openai\"\n",
    "judge_model = GENERATION_MODEL  # Required for HF provider\n",
    "\n",
    "# For OpenAI provider, uncomment and fill these:\n",
    "# provider = \"openai\"\n",
    "# openai_key = \"your_openai_key_here\"\n",
    "# azure_host = \"your_azure_endpoint_here\"\n",
    "\n",
    "success = run_generation_evaluation_script_advanced(\n",
    "    input_file=output_file,\n",
    "    output_file=eval_output_file,\n",
    "    provider=provider,\n",
    "    judge_model=judge_model\n",
    "    # Add if using OpenAI:\n",
    "    # openai_key=openai_key,\n",
    "    # azure_host=azure_host\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nðŸŽ‰ Task C evaluation pipeline completed successfully!\")\n",
    "    print(\"You can now run the analysis cells to see the results.\")\n",
    "else:\n",
    "    print(\"\\nðŸ’¥ Task C evaluation pipeline failed. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d94661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 5 evaluation results...\n",
      "\n",
      "=== Evaluation Metrics Summary ===\n",
      "\n",
      "Average Scores:\n",
      "  RL_F: 0.0000\n",
      "  RB_llm: 0.5000\n",
      "  RB_agg: 0.1656\n",
      "  RL_F_idk: 0.0000\n",
      "  RB_llm_idk: 0.5000\n",
      "  RB_agg_idk: 0.1656\n",
      "\n",
      "Score Distributions:\n",
      "\n",
      "RL_F:\n",
      "count    2.0\n",
      "mean     0.0\n",
      "std      0.0\n",
      "min      0.0\n",
      "25%      0.0\n",
      "50%      0.0\n",
      "75%      0.0\n",
      "max      0.0\n",
      "Name: RL_F, dtype: float64\n",
      "\n",
      "RB_llm:\n",
      "count    5.0\n",
      "mean     0.5\n",
      "std      0.0\n",
      "min      0.5\n",
      "25%      0.5\n",
      "50%      0.5\n",
      "75%      0.5\n",
      "max      0.5\n",
      "Name: RB_llm, dtype: float64\n",
      "\n",
      "RB_agg:\n",
      "count    5.000000\n",
      "mean     0.165648\n",
      "std      0.085967\n",
      "min      0.074338\n",
      "25%      0.098169\n",
      "50%      0.167334\n",
      "75%      0.197689\n",
      "max      0.290710\n",
      "Name: RB_agg, dtype: float64\n",
      "\n",
      "RL_F_idk:\n",
      "count    2.0\n",
      "mean     0.0\n",
      "std      0.0\n",
      "min      0.0\n",
      "25%      0.0\n",
      "50%      0.0\n",
      "75%      0.0\n",
      "max      0.0\n",
      "Name: RL_F_idk, dtype: float64\n",
      "\n",
      "RB_llm_idk:\n",
      "count    5.0\n",
      "mean     0.5\n",
      "std      0.0\n",
      "min      0.5\n",
      "25%      0.5\n",
      "50%      0.5\n",
      "75%      0.5\n",
      "max      0.5\n",
      "Name: RB_llm_idk, dtype: float64\n",
      "\n",
      "RB_agg_idk:\n",
      "count    5.000000\n",
      "mean     0.165648\n",
      "std      0.085967\n",
      "min      0.074338\n",
      "25%      0.098169\n",
      "50%      0.167334\n",
      "75%      0.197689\n",
      "max      0.290710\n",
      "Name: RB_agg_idk, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def analyze_evaluation_results(results_file):\n",
    "    \"\"\"Analyze and display evaluation results\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    with open(results_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Analyzing {len(results)} evaluation results...\")\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics_data = []\n",
    "    for result in results:\n",
    "        task_id = result.get('task_id')\n",
    "        metrics = result.get('metrics', {})\n",
    "        \n",
    "        metric_row = {'task_id': task_id}\n",
    "        for metric_name, metric_values in metrics.items():\n",
    "            if isinstance(metric_values, list) and len(metric_values) > 0:\n",
    "                metric_row[metric_name] = metric_values[0]\n",
    "            else:\n",
    "                metric_row[metric_name] = metric_values\n",
    "        \n",
    "        metrics_data.append(metric_row)\n",
    "    \n",
    "    if metrics_data:\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        print(\"\\n=== Evaluation Metrics Summary ===\")\n",
    "        \n",
    "        # Display average scores for key metrics\n",
    "        key_metrics = ['RL_F', 'RB_llm', 'RB_agg', 'RL_F_idk', 'RB_llm_idk', 'RB_agg_idk']\n",
    "        available_metrics = [m for m in key_metrics if m in metrics_df.columns]\n",
    "        \n",
    "        if available_metrics:\n",
    "            summary = metrics_df[available_metrics].mean()\n",
    "            print(\"\\nAverage Scores:\")\n",
    "            for metric, score in summary.items():\n",
    "                print(f\"  {metric}: {score:.4f}\")\n",
    "        \n",
    "        # Display score distributions\n",
    "        print(\"\\nScore Distributions:\")\n",
    "        for metric in available_metrics:\n",
    "            if metric in metrics_df.columns:\n",
    "                print(f\"\\n{metric}:\")\n",
    "                print(metrics_df[metric].describe())\n",
    "    \n",
    "    return metrics_data\n",
    "\n",
    "# Analyze results\n",
    "metrics_data = analyze_evaluation_results(eval_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f931c04e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Create final report\u001b[39;00m\n\u001b[0;32m     61\u001b[0m report_file \u001b[38;5;241m=\u001b[39m BASE \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrayaan\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_c_final_report.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 62\u001b[0m final_report \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_final_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_output_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m, in \u001b[0;36mcreate_final_report\u001b[1;34m(results_file, output_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m report \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubtask C - Generation with Retrieved Passages (RAG)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(results),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval_setting\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull RAG (top 5 passages)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m }\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_name, values \u001b[38;5;129;01min\u001b[39;00m all_metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     31\u001b[0m     report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m][metric_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mstd(values),\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmin(values),\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmax(values),\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmedian(values)\n\u001b[0;32m     37\u001b[0m     }\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Save report\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\rayaa\\anaconda3\\envs\\rag_gen_eval\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3505\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rayaa\\anaconda3\\envs\\rag_gen_eval\\lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "def create_final_report(results_file, output_path):\n",
    "    \"\"\"Create a comprehensive final report\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    with open(results_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_metrics = {}\n",
    "    for result in results:\n",
    "        metrics = result.get('metrics', {})\n",
    "        for metric_name, metric_values in metrics.items():\n",
    "            if isinstance(metric_values, list) and len(metric_values) > 0:\n",
    "                if metric_name not in all_metrics:\n",
    "                    all_metrics[metric_name] = []\n",
    "                all_metrics[metric_name].append(metric_values[0])\n",
    "    \n",
    "    # Create report\n",
    "    report = {\n",
    "        \"task\": \"Subtask C - Generation with Retrieved Passages (RAG)\",\n",
    "        \"total_tasks\": len(results),\n",
    "        \"evaluation_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        \"metrics_summary\": {},\n",
    "        \"model_used\": GENERATION_MODEL,\n",
    "        \"retrieval_setting\": \"Full RAG (top 5 passages)\"\n",
    "    }\n",
    "    \n",
    "    for metric_name, values in all_metrics.items():\n",
    "        report[\"metrics_summary\"][metric_name] = {\n",
    "            \"mean\": np.mean(values),\n",
    "            \"std\": np.std(values),\n",
    "            \"min\": np.min(values),\n",
    "            \"max\": np.max(values),\n",
    "            \"median\": np.median(values)\n",
    "        }\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"Final report saved to: {output_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== TASK C FINAL REPORT ===\")\n",
    "    print(f\"Total tasks evaluated: {report['total_tasks']}\")\n",
    "    print(f\"Generation model: {report['model_used']}\")\n",
    "    print(f\"Retrieval setting: {report['retrieval_setting']}\")\n",
    "    print(\"\\nKey Metrics:\")\n",
    "    \n",
    "    key_metrics = ['RL_F', 'RB_llm', 'RB_agg', 'RL_F_idk', 'RB_llm_idk', 'RB_agg_idk']\n",
    "    for metric in key_metrics:\n",
    "        if metric in report['metrics_summary']:\n",
    "            stats = report['metrics_summary'][metric]\n",
    "            print(f\"  {metric}: {stats['mean']:.4f} (Â±{stats['std']:.4f})\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Create final report\n",
    "report_file = BASE / 'rayaan' / 'outputs' / 'task_c_final_report.json'\n",
    "final_report = create_final_report(eval_output_file, report_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_gen_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
