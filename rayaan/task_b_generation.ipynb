{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d73896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db0a993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths and models configured\n"
     ]
    }
   ],
   "source": [
    "# Paths setup\n",
    "BASE = Path('c:/Users/rayaa/OneDrive/Documents/VSCode/CSCI5832/Semeval')\n",
    "RAG_TASKS_PATH = BASE / 'human' / 'generation_tasks' / 'RAG.jsonl'\n",
    "CORPUS_PATH = BASE / 'corpora' / 'passage_level' / 'cloud.jsonl'\n",
    "\n",
    "# Model and database setup\n",
    "EMBED_MODEL = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "GENERATION_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "PG_ENV_PATH = BASE / '.pg_env'\n",
    "\n",
    "print('Paths and models configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9cc605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 205 filtered RAG tasks\n"
     ]
    }
   ],
   "source": [
    "def load_rag_tasks(jsonl_path, collection_name=\"mt-rag-ibmcloud-elser-512-100-20240502\"):\n",
    "    \"\"\"Load RAG tasks from JSONL file, filtering by id.Collection.\"\"\"\n",
    "    tasks = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Safely extract Collection field\n",
    "            collection = obj.get(\"Collection\", \"\")\n",
    "\n",
    "            if collection == collection_name:\n",
    "                tasks.append(obj)\n",
    "\n",
    "    return tasks\n",
    "\n",
    "# Load RAG tasks\n",
    "rag_tasks = load_rag_tasks(RAG_TASKS_PATH)\n",
    "print(f\"Loaded {len(rag_tasks)} filtered RAG tasks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d942db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator setup complete\n"
     ]
    }
   ],
   "source": [
    "def extract_conversation_text(task):\n",
    "    \"\"\"Extract the current question from conversation input\"\"\"\n",
    "    input_data = task.get('input', [])\n",
    "    if isinstance(input_data, list) and len(input_data) > 0:\n",
    "        # Get the last user message\n",
    "        for msg in reversed(input_data):\n",
    "            if msg.get('speaker') == 'user':\n",
    "                return msg.get('text', '')\n",
    "    return ''\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "def setup_generator(model_name=GENERATION_MODEL):\n",
    "    \"\"\"Setup the text generation model\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return generator\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up generator: {e}\")\n",
    "        return None\n",
    "\n",
    "# Setup the generator\n",
    "generator = setup_generator()\n",
    "print('Generator setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3147335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generation_prompt(question, contexts, conversation_history=None):\n",
    "    \"\"\"Create a prompt for answer generation using retrieved contexts\"\"\"\n",
    "    \n",
    "    # Build context string\n",
    "    context_text = \"\"\n",
    "    for i, ctx in enumerate(contexts, 1):\n",
    "        context_text += f\"Context {i}: {ctx['text']}\\n\\n\"\n",
    "    \n",
    "    # Build conversation history if available\n",
    "    history = \"\"\n",
    "    if conversation_history:\n",
    "        turns = [\n",
    "            f\"{t['speaker'].capitalize()}: {t['text']}\"\n",
    "            for t in conversation_history\n",
    "        ]\n",
    "        history = \"Conversation:\\n\" + \"\\n\".join(turns) + \"\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"You are a concise conversational assistant.\n",
    "\n",
    "Use ONLY the information found in the contexts.  \n",
    "If the answer is not in the contexts, say exactly: **\"The contexts do not contain the answer.\"**\n",
    "\n",
    "Rules:\n",
    "- Do NOT explain your reasoning.\n",
    "- Do NOT mention the instructions.\n",
    "- Do NOT invent information.\n",
    "- Answer in a single short, natural paragraph (1â€“2 sentences).\n",
    "- No meta-commentary (e.g., \"I will now...\").\n",
    "\n",
    "{history}Contexts:\n",
    "{context_text}\n",
    "\n",
    "User: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_answer(prompt, generator):\n",
    "    \"\"\"Generate answer using the language model\"\"\"\n",
    "    if generator is None:\n",
    "        return \"[Generation model not available]\"\n",
    "    \n",
    "    try:\n",
    "        outputs = generator(\n",
    "            prompt,\n",
    "            return_full_text=False,\n",
    "            pad_token_id=generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        if outputs and len(outputs) > 0:\n",
    "            return outputs[0]['generated_text'].strip()\n",
    "        else:\n",
    "            return \"[No response generated]\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generation: {e}\")\n",
    "        return f\"[Generation error: {e}]\"\n",
    "    \n",
    "def trim_to_token_limit(text, tokenizer, max_length=4000):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) <= max_length:\n",
    "        return text\n",
    "    # Keep last max_length tokens so the question stays\n",
    "    trimmed = tokenizer.decode(tokens[-max_length:])\n",
    "    return trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af276f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e321cc577d74a64bc480c9aee160d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing RAG tasks:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 results to c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_b_rag_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "def run_task_b_rag(tasks, generator, output_path, do_subset=False):\n",
    "    \"\"\"Run full Task B pipeline: reference generation\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if (do_subset):\n",
    "        tasks = tasks[:5]\n",
    "    \n",
    "    for task in tqdm(tasks, desc=\"Processing RAG tasks\"):\n",
    "        # Extract current question\n",
    "        current_question = extract_conversation_text(task)\n",
    "        \n",
    "        if not current_question:\n",
    "            print(f\"Warning: No question found for task {task.get('task_id')}\")\n",
    "            continue\n",
    "\n",
    "        provided_contexts = task.get('contexts', [])\n",
    "\n",
    "        formatted_contexts = []\n",
    "        for i, ctx in enumerate(provided_contexts):\n",
    "            if isinstance(ctx, dict):\n",
    "                formatted_contexts.append({\n",
    "                    'document_id': ctx.get('document_id', f'provided_{i}'),\n",
    "                    'text': ctx.get('text', ''),\n",
    "                    'score': 1.0  # All provided contexts get max score\n",
    "                })\n",
    "            else:\n",
    "                # If context is just text\n",
    "                formatted_contexts.append({\n",
    "                    'document_id': f'provided_{i}',\n",
    "                    'text': str(ctx),\n",
    "                    'score': 1.0\n",
    "                })\n",
    "        \n",
    "        # Get conversation history (all but last user message)\n",
    "        conversation_history = []\n",
    "        input_data = task.get('input', [])\n",
    "        if isinstance(input_data, list):\n",
    "            # Include all but the last user message (current question)\n",
    "            found_last_user = False\n",
    "            for msg in reversed(input_data):\n",
    "                if msg.get('speaker') == 'user' and not found_last_user:\n",
    "                    found_last_user = True\n",
    "                    continue\n",
    "                conversation_history.insert(0, msg)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = create_generation_prompt(current_question, formatted_contexts, conversation_history)\n",
    "        prompt = trim_to_token_limit(prompt, generator.tokenizer, max_length=4000)\n",
    "        generated_answer = generate_answer(prompt, generator)\n",
    "        \n",
    "        # Prepare result in evaluation format\n",
    "        result_task = task.copy()\n",
    "        \n",
    "        # Add prediction in the format expected by evaluation script\n",
    "        result_task['predictions'] = [{\n",
    "            'text': generated_answer\n",
    "        }]\n",
    "        \n",
    "        results.append(result_task)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(output_path).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(results)} results to {output_path}\")\n",
    "    return results\n",
    "\n",
    "# Run Task B\n",
    "output_file = BASE / 'rayaan' / 'outputs' / 'task_b_rag_predictions.jsonl'\n",
    "task_b_results = run_task_b_rag(rag_tasks, generator, output_file, do_subset=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
