{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d73896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db0a993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths and models configured\n"
     ]
    }
   ],
   "source": [
    "# Paths setup\n",
    "BASE = Path('c:/Users/rayaa/OneDrive/Documents/VSCode/CSCI5832/Semeval')\n",
    "RAG_TASKS_PATH = BASE / 'human' / 'generation_tasks' / 'RAG.jsonl'\n",
    "CORPUS_PATH = BASE / 'corpora' / 'passage_level' / 'cloud.jsonl'\n",
    "\n",
    "# Model and database setup\n",
    "EMBED_MODEL = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "GENERATION_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "PG_ENV_PATH = BASE / '.pg_env'\n",
    "\n",
    "print('Paths and models configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9cc605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 205 filtered RAG tasks\n"
     ]
    }
   ],
   "source": [
    "def load_rag_tasks(jsonl_path, collection_name=\"mt-rag-ibmcloud-elser-512-100-20240502\"):\n",
    "    \"\"\"Load RAG tasks from JSONL file, filtering by id.Collection.\"\"\"\n",
    "    tasks = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Safely extract Collection field\n",
    "            collection = obj.get(\"Collection\", \"\")\n",
    "\n",
    "            if collection == collection_name:\n",
    "                tasks.append(obj)\n",
    "\n",
    "    return tasks\n",
    "\n",
    "# Load RAG tasks\n",
    "rag_tasks = load_rag_tasks(RAG_TASKS_PATH)\n",
    "print(f\"Loaded {len(rag_tasks)} filtered RAG tasks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d942db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator setup complete\n"
     ]
    }
   ],
   "source": [
    "def extract_conversation_text(task):\n",
    "    \"\"\"Extract the current question from conversation input\"\"\"\n",
    "    input_data = task.get('input', [])\n",
    "    if isinstance(input_data, list) and len(input_data) > 0:\n",
    "        # Get the last user message\n",
    "        for msg in reversed(input_data):\n",
    "            if msg.get('speaker') == 'user':\n",
    "                return msg.get('text', '')\n",
    "    return ''\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "def setup_generator(model_name=GENERATION_MODEL):\n",
    "    \"\"\"Setup the text generation model\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return generator\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up generator: {e}\")\n",
    "        return None\n",
    "\n",
    "# Setup the generator\n",
    "generator = setup_generator()\n",
    "print('Generator setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3147335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generation_prompt(question, contexts, conversation_history=None):\n",
    "    \"\"\"Create a prompt for answer generation using retrieved contexts\"\"\"\n",
    "    \n",
    "    # Build context string\n",
    "    context_text = \"\"\n",
    "    for i, ctx in enumerate(contexts, 1):\n",
    "        context_text += f\"Context {i}: {ctx['text']}\\n\\n\"\n",
    "    \n",
    "    # Build conversation history if available\n",
    "    history = \"\"\n",
    "    if conversation_history:\n",
    "        turns = [\n",
    "            f\"{t['speaker'].capitalize()}: {t['text']}\"\n",
    "            for t in conversation_history\n",
    "        ]\n",
    "        history = \"Conversation:\\n\" + \"\\n\".join(turns) + \"\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"You are a concise conversational assistant.\n",
    "\n",
    "Use ONLY the information found in the contexts.  \n",
    "If the answer is not in the contexts, say exactly: **\"The contexts do not contain the answer.\"**\n",
    "\n",
    "Rules:\n",
    "- Do NOT explain your reasoning.\n",
    "- Do NOT mention the instructions.\n",
    "- Do NOT invent information.\n",
    "- Answer in a single short, natural paragraph (1‚Äì2 sentences).\n",
    "- No meta-commentary (e.g., \"I will now...\").\n",
    "\n",
    "{history}Contexts:\n",
    "{context_text}\n",
    "\n",
    "User: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_answer(prompt, generator):\n",
    "    \"\"\"Generate answer using the language model\"\"\"\n",
    "    if generator is None:\n",
    "        return \"[Generation model not available]\"\n",
    "    \n",
    "    try:\n",
    "        outputs = generator(\n",
    "            prompt,\n",
    "            return_full_text=False,\n",
    "            pad_token_id=generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        if outputs and len(outputs) > 0:\n",
    "            return outputs[0]['generated_text'].strip()\n",
    "        else:\n",
    "            return \"[No response generated]\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generation: {e}\")\n",
    "        return f\"[Generation error: {e}]\"\n",
    "    \n",
    "def trim_to_token_limit(text, tokenizer, max_length=4000):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) <= max_length:\n",
    "        return text\n",
    "    # Keep last max_length tokens so the question stays\n",
    "    trimmed = tokenizer.decode(tokens[-max_length:])\n",
    "    return trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af276f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad944c99c6b242ed906481a0aa47a0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing RAG tasks:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 results to c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_b_rag_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "def run_task_b_rag(tasks, generator, output_path, do_subset=False):\n",
    "    \"\"\"Run full Task B pipeline: reference generation\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if (do_subset):\n",
    "        tasks = tasks[:100]\n",
    "    \n",
    "    for task in tqdm(tasks, desc=\"Processing RAG tasks\"):\n",
    "        # Extract current question\n",
    "        current_question = extract_conversation_text(task)\n",
    "        \n",
    "        if not current_question:\n",
    "            print(f\"Warning: No question found for task {task.get('task_id')}\")\n",
    "            continue\n",
    "\n",
    "        provided_contexts = task.get('contexts', [])\n",
    "\n",
    "        formatted_contexts = []\n",
    "        for i, ctx in enumerate(provided_contexts):\n",
    "            if isinstance(ctx, dict):\n",
    "                formatted_contexts.append({\n",
    "                    'document_id': ctx.get('document_id', f'provided_{i}'),\n",
    "                    'text': ctx.get('text', ''),\n",
    "                    'score': 1.0  # All provided contexts get max score\n",
    "                })\n",
    "            else:\n",
    "                # If context is just text\n",
    "                formatted_contexts.append({\n",
    "                    'document_id': f'provided_{i}',\n",
    "                    'text': str(ctx),\n",
    "                    'score': 1.0\n",
    "                })\n",
    "        \n",
    "        # Get conversation history (all but last user message)\n",
    "        conversation_history = []\n",
    "        input_data = task.get('input', [])\n",
    "        if isinstance(input_data, list):\n",
    "            # Include all but the last user message (current question)\n",
    "            found_last_user = False\n",
    "            for msg in reversed(input_data):\n",
    "                if msg.get('speaker') == 'user' and not found_last_user:\n",
    "                    found_last_user = True\n",
    "                    continue\n",
    "                conversation_history.insert(0, msg)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = create_generation_prompt(current_question, formatted_contexts, conversation_history)\n",
    "        prompt = trim_to_token_limit(prompt, generator.tokenizer, max_length=4000)\n",
    "        generated_answer = generate_answer(prompt, generator)\n",
    "        \n",
    "        # Prepare result in evaluation format\n",
    "        result_task = task.copy()\n",
    "        \n",
    "        # Add prediction in the format expected by evaluation script\n",
    "        result_task['predictions'] = [{\n",
    "            'text': generated_answer\n",
    "        }]\n",
    "        \n",
    "        results.append(result_task)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(output_path).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(results)} results to {output_path}\")\n",
    "    return results\n",
    "\n",
    "# Run Task B\n",
    "output_file = BASE / 'rayaan' / 'outputs' / 'task_b_rag_predictions.jsonl'\n",
    "task_b_results = run_task_b_rag(rag_tasks, generator, output_file, do_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd52b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING GENERATION EVALUATION PIPELINE\n",
      "============================================================\n",
      "üìÅ Input file: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_b_rag_predictions.jsonl (100 tasks)\n",
      "üìÅ Output directory: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\n",
      "üîß Evaluation script: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\scripts\\evaluation\\run_generation_eval.py\n",
      "ü§ñ Using HF judge model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "üöÄ Command: python c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\scripts\\evaluation\\run_generation_eval.py --input c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_b_rag_predictions.jsonl --output c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_b_evaluation_results.jsonl --algorithmic_evaluators c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\scripts\\evaluation\\config.yaml --provider hf --judge_model Qwen/Qwen2.5-0.5B-Instruct\n",
      "\n",
      "‚è≥ Starting evaluation... This may take a while...\n"
     ]
    }
   ],
   "source": [
    "output_file = BASE / 'rayaan' / 'outputs' / 'task_b_rag_predictions.jsonl'\n",
    "\n",
    "def run_generation_evaluation_script_advanced(input_file, output_file, provider=\"hf\", judge_model=None, openai_key=None, azure_host=None):\n",
    "    \"\"\"Run the generation evaluation pipeline with comprehensive error handling\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING GENERATION EVALUATION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Validate inputs\n",
    "    input_path = Path(input_file)\n",
    "    if not input_path.exists():\n",
    "        print(f\"‚ùå ERROR: Input file not found at {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Count input tasks\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        input_count = sum(1 for line in f if line.strip())\n",
    "    print(f\"üìÅ Input file: {input_path} ({input_count} tasks)\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_path = Path(output_file)\n",
    "    output_dir = output_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"üìÅ Output directory: {output_dir}\")\n",
    "    \n",
    "    # Locate evaluation script\n",
    "    eval_script_path = BASE / 'scripts' / 'evaluation' / 'run_generation_eval.py'\n",
    "    if not eval_script_path.exists():\n",
    "        print(f\"‚ùå ERROR: Evaluation script not found at {eval_script_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Locate config file\n",
    "    config_path = BASE / 'scripts' / 'evaluation' / 'config.yaml'\n",
    "    if not config_path.exists():\n",
    "        print(f\"‚ùå ERROR: Config file not found at {config_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üîß Evaluation script: {eval_script_path}\")\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [\n",
    "        'python', str(eval_script_path),\n",
    "        '--input', str(input_path),\n",
    "        '--output', str(output_path),\n",
    "        '--algorithmic_evaluators', str(config_path),\n",
    "        '--provider', provider\n",
    "    ]\n",
    "    \n",
    "    # Add provider-specific arguments\n",
    "    if provider == \"hf\" and judge_model:\n",
    "        cmd.extend(['--judge_model', judge_model])\n",
    "        print(f\"ü§ñ Using HF judge model: {judge_model}\")\n",
    "    elif provider == \"openai\":\n",
    "        if openai_key:\n",
    "            cmd.extend(['--openai_key', openai_key])\n",
    "            print(\"üîë OpenAI key provided\")\n",
    "        if azure_host:\n",
    "            cmd.extend(['--azure_host', azure_host])\n",
    "            print(f\"üåê Azure host: {azure_host}\")\n",
    "    \n",
    "    print(f\"üöÄ Command: {' '.join(cmd)}\")\n",
    "    print(\"\\n‚è≥ Starting evaluation... This may take a while...\")\n",
    "    \n",
    "    try:\n",
    "        # Run the evaluation script with timeout\n",
    "        import subprocess\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=17200)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"‚è±Ô∏è  Execution time: {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)\")\n",
    "        \n",
    "        # Process results\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if result.stdout:\n",
    "            print(\"üìã Script output:\")\n",
    "            print(result.stdout)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Evaluation completed successfully!\")\n",
    "            \n",
    "            # Verify output file\n",
    "            if output_path.exists():\n",
    "                with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                    output_count = sum(1 for line in f if line.strip())\n",
    "                print(f\"üìä Results: {output_count}/{input_count} tasks evaluated\")\n",
    "                print(f\"üíæ Results saved to: {output_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå ERROR: Output file was not created\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚ùå Evaluation failed with return code: {result.returncode}\")\n",
    "            if result.stderr:\n",
    "                print(\"üî¥ Error details:\")\n",
    "                print(result.stderr)\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ ERROR: Evaluation script timed out after 2 hours\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"üí• Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run evaluation on Task B results\n",
    "eval_output_file = BASE / 'rayaan' / 'outputs' / 'task_b_evaluation_results.jsonl'\n",
    "\n",
    "# Configuration - Choose your setup\n",
    "provider = \"hf\"  # Options: \"hf\" or \"openai\"\n",
    "judge_model = GENERATION_MODEL  # Required for HF provider\n",
    "\n",
    "# For OpenAI provider, uncomment and fill these:\n",
    "# provider = \"openai\"\n",
    "# openai_key = \"your_openai_key_here\"\n",
    "# azure_host = \"your_azure_endpoint_here\"\n",
    "\n",
    "success = run_generation_evaluation_script_advanced(\n",
    "    input_file=output_file,\n",
    "    output_file=eval_output_file,\n",
    "    provider=provider,\n",
    "    judge_model=judge_model\n",
    "    # Add if using OpenAI:\n",
    "    # openai_key=openai_key,\n",
    "    # azure_host=azure_host\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Task B evaluation pipeline completed successfully!\")\n",
    "    print(\"You can now run the analysis cells to see the results.\")\n",
    "else:\n",
    "    print(\"\\nüí• Task B evaluation pipeline failed. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e8e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 100 evaluation results...\n",
      "\n",
      "=== Evaluation Metrics Summary ===\n",
      "\n",
      "Average Scores:\n",
      "  RB_agg: 0.2636\n",
      "\n",
      "Score Distributions:\n",
      "\n",
      "RB_agg:\n",
      "count    100.000000\n",
      "mean       0.263619\n",
      "std        0.082125\n",
      "min        0.061627\n",
      "25%        0.219482\n",
      "50%        0.264770\n",
      "75%        0.314496\n",
      "max        0.515473\n",
      "Name: RB_agg, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def analyze_evaluation_results(results_file):\n",
    "    \"\"\"Analyze and display evaluation results\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    with open(results_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Analyzing {len(results)} evaluation results...\")\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics_data = []\n",
    "    for result in results:\n",
    "        task_id = result.get('task_id')\n",
    "        metrics = result.get('metrics', {})\n",
    "        \n",
    "        metric_row = {'task_id': task_id}\n",
    "        for metric_name, metric_values in metrics.items():\n",
    "            if isinstance(metric_values, list) and len(metric_values) > 0:\n",
    "                metric_row[metric_name] = metric_values[0]\n",
    "            else:\n",
    "                metric_row[metric_name] = metric_values\n",
    "        \n",
    "        metrics_data.append(metric_row)\n",
    "    \n",
    "    if metrics_data:\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        print(\"\\n=== Evaluation Metrics Summary ===\")\n",
    "        \n",
    "        # Display average scores for key metrics\n",
    "        key_metrics = ['RL_F', 'RB_llm', 'RB_agg', 'RL_F_idk', 'RB_llm_idk', 'RB_agg_idk']\n",
    "        available_metrics = [m for m in key_metrics if m in metrics_df.columns]\n",
    "        \n",
    "        if available_metrics:\n",
    "            summary = metrics_df[available_metrics].mean()\n",
    "            print(\"\\nAverage Scores:\")\n",
    "            for metric, score in summary.items():\n",
    "                print(f\"  {metric}: {score:.4f}\")\n",
    "        \n",
    "        # Display score distributions\n",
    "        print(\"\\nScore Distributions:\")\n",
    "        for metric in available_metrics:\n",
    "            if metric in metrics_df.columns:\n",
    "                print(f\"\\n{metric}:\")\n",
    "                print(metrics_df[metric].describe())\n",
    "    \n",
    "    return metrics_data\n",
    "\n",
    "# Analyze results\n",
    "metrics_data = analyze_evaluation_results(eval_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce98566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final report saved to: c:\\Users\\rayaa\\OneDrive\\Documents\\VSCode\\CSCI5832\\Semeval\\rayaan\\outputs\\task_b_final_report.json\n",
      "\n",
      "=== TASK B FINAL REPORT ===\n",
      "Total tasks evaluated: 100\n",
      "Generation model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Retrieval setting: Reference RAG (top 5 passages)\n",
      "\n",
      "‚ö†Ô∏è  Warnings:\n",
      "  - Error calculating stats for 'idk_eval': ufunc 'add' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> None\n",
      "\n",
      "Key Metrics:\n",
      "  RB_agg: 0.2636 (¬±0.0817) [valid: 100/100]\n"
     ]
    }
   ],
   "source": [
    "def create_final_report(results_file, output_path):\n",
    "    \"\"\"Create a comprehensive final report\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    with open(results_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_metrics = {}\n",
    "    for result in results:\n",
    "        metrics = result.get('metrics', {})\n",
    "        for metric_name, metric_values in metrics.items():\n",
    "            if isinstance(metric_values, list) and len(metric_values) > 0:\n",
    "                if metric_name not in all_metrics:\n",
    "                    all_metrics[metric_name] = []\n",
    "                all_metrics[metric_name].append(metric_values[0])\n",
    "    \n",
    "    # Create report\n",
    "    report = {\n",
    "        \"task\": \"Subtask B - Generation with Reference Passages (RAG)\",\n",
    "        \"total_tasks\": len(results),\n",
    "        \"evaluation_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        \"metrics_summary\": {},\n",
    "        \"model_used\": GENERATION_MODEL,\n",
    "        \"retrieval_setting\": \"Reference RAG (top 5 passages)\",\n",
    "        \"warnings\": []\n",
    "    }\n",
    "    \n",
    "    for metric_name, values in all_metrics.items():\n",
    "        # Filter out None values before calculating statistics\n",
    "        clean_values = [v for v in values if v is not None]\n",
    "        \n",
    "        if not clean_values:  # If all values are None\n",
    "            report[\"warnings\"].append(f\"Metric '{metric_name}' has no valid values (all None)\")\n",
    "            report[\"metrics_summary\"][metric_name] = {\n",
    "                \"mean\": None,\n",
    "                \"std\": None,\n",
    "                \"min\": None,\n",
    "                \"max\": None,\n",
    "                \"median\": None,\n",
    "                \"count\": 0,\n",
    "                \"total_count\": len(values),\n",
    "                \"valid_count\": 0\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Calculate statistics safely and convert to native Python types\n",
    "        try:\n",
    "            stats_dict = {\n",
    "                \"mean\": float(np.mean(clean_values)) if np.mean(clean_values) is not None else None,\n",
    "                \"std\": float(np.std(clean_values)) if np.std(clean_values) is not None else None,\n",
    "                \"min\": float(np.min(clean_values)) if np.min(clean_values) is not None else None,\n",
    "                \"max\": float(np.max(clean_values)) if np.max(clean_values) is not None else None,\n",
    "                \"median\": float(np.median(clean_values)) if np.median(clean_values) is not None else None,\n",
    "                \"count\": len(clean_values),\n",
    "                \"total_count\": len(values),\n",
    "                \"valid_count\": len(clean_values)\n",
    "            }\n",
    "            \n",
    "            # Handle potential NaN values\n",
    "            for key in [\"mean\", \"std\", \"min\", \"max\", \"median\"]:\n",
    "                if stats_dict[key] is not None and np.isnan(stats_dict[key]):\n",
    "                    stats_dict[key] = None\n",
    "                    report[\"warnings\"].append(f\"Metric '{metric_name}' has NaN for {key}\")\n",
    "            \n",
    "            report[\"metrics_summary\"][metric_name] = stats_dict\n",
    "            \n",
    "        except (TypeError, ValueError) as e:\n",
    "            report[\"warnings\"].append(f\"Error calculating stats for '{metric_name}': {str(e)}\")\n",
    "            report[\"metrics_summary\"][metric_name] = {\n",
    "                \"mean\": None,\n",
    "                \"std\": None,\n",
    "                \"min\": None,\n",
    "                \"max\": None,\n",
    "                \"median\": None,\n",
    "                \"count\": len(clean_values),\n",
    "                \"total_count\": len(values),\n",
    "                \"valid_count\": len(clean_values),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        \n",
    "        # Optional: add warning if some values were None\n",
    "        if len(clean_values) < len(values):\n",
    "            report[\"warnings\"].append(\n",
    "                f\"Metric '{metric_name}' had {len(values) - len(clean_values)} None values (ignored in stats)\"\n",
    "            )\n",
    "    \n",
    "    # Convert report to JSON-serializable format\n",
    "    def convert_to_serializable(obj):\n",
    "        \"\"\"Recursively convert numpy types to native Python types\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        elif obj is pd.NaT or pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Apply conversion\n",
    "    serializable_report = convert_to_serializable(report)\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Final report saved to: {output_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== TASK B FINAL REPORT ===\")\n",
    "    print(f\"Total tasks evaluated: {report['total_tasks']}\")\n",
    "    print(f\"Generation model: {report['model_used']}\")\n",
    "    print(f\"Retrieval setting: {report['retrieval_setting']}\")\n",
    "    \n",
    "    # Show warnings if any\n",
    "    if report['warnings']:\n",
    "        print(\"\\n‚ö†Ô∏è  Warnings:\")\n",
    "        for warning in report['warnings'][:10]:  # Show only first 10 warnings\n",
    "            print(f\"  - {warning}\")\n",
    "        if len(report['warnings']) > 10:\n",
    "            print(f\"  ... and {len(report['warnings']) - 10} more warnings\")\n",
    "    \n",
    "    print(\"\\nKey Metrics:\")\n",
    "    \n",
    "    key_metrics = ['RL_F', 'RB_llm', 'RB_agg', 'RL_F_idk', 'RB_llm_idk', 'RB_agg_idk']\n",
    "    for metric in key_metrics:\n",
    "        if metric in report['metrics_summary']:\n",
    "            stats = report['metrics_summary'][metric]\n",
    "            if stats.get('mean') is not None and not np.isnan(stats['mean']):\n",
    "                std_val = stats.get('std', 0)\n",
    "                if std_val is not None and not np.isnan(std_val):\n",
    "                    print(f\"  {metric}: {stats['mean']:.4f} (¬±{std_val:.4f}) [valid: {stats.get('valid_count', 0)}/{stats.get('total_count', 0)}]\")\n",
    "                else:\n",
    "                    print(f\"  {metric}: {stats['mean']:.4f} (¬±N/A) [valid: {stats.get('valid_count', 0)}/{stats.get('total_count', 0)}]\")\n",
    "            else:\n",
    "                print(f\"  {metric}: No valid values [valid: {stats.get('valid_count', 0)}/{stats.get('total_count', 0)}]\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Create final report\n",
    "report_file = BASE / 'rayaan' / 'outputs' / 'task_b_final_report.json'\n",
    "final_report = create_final_report(eval_output_file, report_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
